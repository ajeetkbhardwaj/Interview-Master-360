### Distributed Training Model

Distributed training is a model training paradigm that involves spreading training workload across multiple worker nodes, therefore significantly improving the speed of training and model accuracy. We can train any model using it like it's more usefull for large models and compute demanding task as deep learning.

1. [Distributed Data Parallel(DDP) in PyTorch](https://pytorch.org/tutorials/beginner/ddp_series_intro.html?utm_source=distr_landing&utm_medium=ddp_series_intro)
2. [GitHub Repos for DDP tutorial](https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series)
3. [Distributed and Parallel Training Tutorials â€” PyTorch Tutorials 2.5.0+cu124 documentation](https://pytorch.org/tutorials//distributed/home.html#learn-ddp))
